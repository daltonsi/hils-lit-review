@inproceedings{abadi_deep_2016,
 abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a reﬁned analysis of privacy costs within the framework of diﬀerential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training eﬃciency, and model quality.},
 address = {Vienna Austria},
 author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
 booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
 doi = {10.1145/2976749.2978318},
 isbn = {978-1-4503-4139-4},
 keywords = {moments-accountant},
 language = {en},
 month = {October},
 pages = {308--318},
 publisher = {ACM},
 title = {Deep {Learning} with {Differential} {Privacy}},
 url = {https://dl.acm.org/doi/10.1145/2976749.2978318},
 urldate = {2024-03-05},
 year = {2016}
}

@article{achuthan_leveraging_2022,
 author = {Achuthan, Srisairam and Chatterjee, Rishov and Kotnala, Sourabh and Mohanty, Atish and Bhattacharya, Supriyo and Salgia, Ravi and Kulkarni, Prakash},
 doi = {10.1007/s12038-022-00278-3},
 issn = {0973-7138},
 journal = {Journal of Biosciences},
 language = {en},
 month = {September},
 number = {3},
 pages = {43},
 title = {Leveraging deep learning algorithms for synthetic data generation to design and analyze biological networks},
 url = {https://link.springer.com/10.1007/s12038-022-00278-3},
 urldate = {2024-06-07},
 volume = {47},
 year = {2022}
}

@article{al_aziz_differentially_2022,
 author = {Al Aziz, M.M. and Ahmed, T. and Faequa, T. and Jiang, X. and Yao, Y. and Mohammed, N.},
 doi = {10.1145/3469035},
 journal = {ACM Transactions on Computing for Healthcare},
 number = {1},
 title = {Differentially {Private} {Medical} {Texts} {Generation} {Using} {Generative} {Neural} {Networks}},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125710436&doi=10.1145%2f3469035&partnerID=40&md5=bf7a70d88b2b1576aa4ab9e37cad1358},
 volume = {3},
 year = {2022}
}

@article{alam_fedsepsis_2023,
 author = {Alam, M.U. and Rahmani, R.},
 doi = {10.3390/s23020970},
 journal = {Sensors},
 number = {2},
 title = {{FedSepsis}: {A} {Federated} {Multi}-{Modal} {Deep} {Learning}-{Based} {Internet} of {Medical} {Things} {Application} for {Early} {Detection} of {Sepsis} from {Electronic} {Health} {Records} {Using} {Raspberry} {Pi} and {Jetson} {Nano} {Devices}},
 url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146400657&doi=10.3390%2fs23020970&partnerID=40&md5=4859242161abf4048bf79828dea3563d},
 volume = {23},
 year = {2023}
}

@inproceedings{berg_impact_2020,
 abstract = {The impact of de-identiﬁcation on data quality and, in particular, utility for developing models for downstream tasks has been more thoroughly studied for structured data than for unstructured text. While previous studies indicate that text de-identiﬁcation has a limited impact on models for downstream tasks, it remains unclear what the impact is with various levels and forms of de-identiﬁcation, in particular concerning the trade-off between precision and recall. In this paper, the impact of deidentiﬁcation is studied on downstream named entity recognition in Swedish clinical text. The results indicate that de-identiﬁcation models with moderate to high precision lead to similar downstream performance, while low precision has a substantial negative impact. Furthermore, different strategies for concealing sensitive information affect performance to different degrees, ranging from pseudonymisation having a low impact to the removal of entire sentences with sensitive information having a high impact. This study indicates that it is possible to increase the recall of models for identifying sensitive information without negatively affecting the use of de-identiﬁed text data for training models for clinical named entity recognition; however, there is ultimately a trade-off between the level of de-identiﬁcation and the subsequent utility of the data.},
 address = {Online},
 author = {Berg, Hanna and Henriksson, Aron and Dalianis, Hercules},
 booktitle = {Proceedings of the 11th {International} {Workshop} on {Health} {Text} {Mining} and {Information} {Analysis}},
 doi = {10.18653/v1/2020.louhi-1.1},
 language = {en},
 pages = {1--11},
 publisher = {Association for Computational Linguistics},
 title = {The {Impact} of {De}-identification on {Downstream} {Named} {Entity} {Recognition} in {Clinical} {Text}},
 url = {https://www.aclweb.org/anthology/2020.louhi-1.1},
 urldate = {2024-01-02},
 year = {2020}
}

@inproceedings{bottou_large-scale_2011,
 abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively di↵erent tradeo↵s for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically e cient after a single pass on the training set.},
 author = {Bottou, Leon},
 booktitle = {Statistical {Learning} and {Data} {Science}},
 doi = {10.1201/b11429-6},
 isbn = {978-0-429-10768-9},
 language = {en},
 month = {December},
 pages = {33--42},
 publisher = {Chapman and Hall/CRC},
 title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent} {Léon} {Bottou}},
 url = {https://www.taylorfrancis.com/books/9781439867648/chapters/10.1201/b11429-6},
 urldate = {2024-04-21},
 year = {2011}
}

@misc{brown_what_2022,
 abstract = {Natural language reﬂects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and diﬀerential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.},
 author = {Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tramèr, Florian},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2202.05520 [cs, stat]},
 publisher = {arXiv},
 title = {What {Does} it {Mean} for a {Language} {Model} to {Preserve} {Privacy}?},
 url = {http://arxiv.org/abs/2202.05520},
 urldate = {2024-02-23},
 year = {2022}
}

@misc{carlini_extracting_2021,
 abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.},
 author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
 keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2012.07805 [cs]},
 publisher = {arXiv},
 title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
 url = {http://arxiv.org/abs/2012.07805},
 urldate = {2024-01-02},
 year = {2021}
}

@misc{carlini_quantifying_2023,
 abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization signiﬁcantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we ﬁnd the situation becomes more complicated when generalizing these results across model families. On the whole, we ﬁnd that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
 author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {March},
 note = {arXiv:2202.07646 [cs]},
 publisher = {arXiv},
 title = {Quantifying {Memorization} {Across} {Neural} {Language} {Models}},
 url = {http://arxiv.org/abs/2202.07646},
 urldate = {2024-02-23},
 year = {2023}
}

@misc{carlini_secret_2019,
 abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models—a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users’ private messages), this methodology can beneﬁt privacy by allowing deep-learning practitioners to select means of training that minimize such memorization.},
 author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, exposure},
 language = {en},
 month = {July},
 note = {arXiv:1802.08232 [cs]},
 publisher = {arXiv},
 shorttitle = {The {Secret} {Sharer}},
 title = {The {Secret} {Sharer}: {Evaluating} and {Testing} {Unintended} {Memorization} in {Neural} {Networks}},
 url = {http://arxiv.org/abs/1802.08232},
 urldate = {2023-07-19},
 year = {2019}
}

@misc{chen_killing_2021,
 abstract = {The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classiﬁcation to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating ﬁne-tuned BERT-based models as APIs. However, BERTbased APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the ﬁne-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by ﬁrst presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERTbased APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.},
 author = {Chen, Chen and He, Xuanli and Lyu, Lingjuan and Wu, Fangzhao},
 keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
 language = {en},
 month = {December},
 note = {arXiv:2105.10909 [cs]},
 publisher = {arXiv},
 shorttitle = {Killing {One} {Bird} with {Two} {Stones}},
 title = {Killing {One} {Bird} with {Two} {Stones}: {Model} {Extraction} and {Attribute} {Inference} {Attacks} against {BERT}-based {APIs}},
 url = {http://arxiv.org/abs/2105.10909},
 urldate = {2023-12-28},
 year = {2021}
}

@article{coiera_standard_2023,
 abstract = {Objective: This article proposes a framework to support the scientiﬁc research of standards so that they can be better measured, evaluated, and designed.
Methods: Beginning with the notion of common models, the framework describes the general standard problem—the seeming impossibility of creating a singular, persistent, and deﬁnitive standard which is not subject to change over time in an open system.
Results: The standard problem arises from uncertainty driven by variations in operating context, standard quality, differences in implementation, and drift over time. As a result, ﬁtting work using conformance services is needed to repair these gaps between a standard and what is required for real-world use. To guide standards design and repair, a framework for measuring performance in context is suggested, based on signal detection theory and technomarkers. Based on the type of common model in operation, different conformance strategies are identiﬁed: (1) Universal conformance (all agents access the same standard); (2) Mediated conformance (an interoperability layer supports heterogeneous agents); and (3) Localized conformance (autonomous adaptive agents manage their own needs). Conformance methods include incremental design, modular design, adaptors, and creating interactive and adaptive agents. Discussion: Machine learning should have a major role in adaptive ﬁtting. Research to guide the choice and design of conformance services may focus on the stability and homogeneity of shared tasks, and whether common models are shared ahead of time or adjusted at task time.
Conclusion: This analysis conceptually decouples interoperability and standardization. While standards facilitate interoperability, interoperability is achievable without standardization.},
 author = {Coiera, Enrico},
 doi = {10.1093/jamia/ocad176},
 issn = {1067-5027, 1527-974X},
 journal = {Journal of the American Medical Informatics Association},
 language = {en},
 month = {November},
 number = {12},
 pages = {2086--2097},
 title = {The standard problem},
 url = {https://academic.oup.com/jamia/article/30/12/2086/7257695},
 urldate = {2024-02-21},
 volume = {30},
 year = {2023}
}

@article{de_rosa_survey_2021,
 abstract = {This work presents a thorough review concerning recent studies and text generation advancements using Generative Adversarial Networks. The usage of adversarial learning for text generation is promising as it provides alternatives to generate the so-called “natural” language. Nevertheless, adversarial text generation is not a simple task as its foremost architecture, the Generative Adversarial Networks, were designed to cope with continuous information (image) instead of discrete data (text). Thus, most works are based on three possible options, i.e., Gumbel-Softmax diﬀerentiation, Reinforcement Learning, and modiﬁed training objectives. All alternatives are reviewed in this survey as they present the most recent approaches for generating text using adversarial-based techniques. The selected works were taken from renowned databases, such as Science Direct, IEEEXplore, Springer, Association for Computing Machinery, and arXiv, whereas each selected work has been critically analyzed and assessed to present its objective, methodology, and experimental results.},
 author = {de Rosa, Gustavo Henrique and Papa, João Paulo},
 doi = {10.1016/j.patcog.2021.108098},
 issn = {00313203},
 journal = {Pattern Recognition},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2212.11119 [cs]},
 pages = {108098},
 title = {A survey on text generation using generative adversarial networks},
 url = {http://arxiv.org/abs/2212.11119},
 urldate = {2024-06-28},
 volume = {119},
 year = {2021}
}

@article{dean_large_2012,
 abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
 keywords = {lr-supplemental},
 language = {en},
 title = {Large {Scale} {Distributed} {Deep} {Networks}},
 year = {2012}
}

@misc{devlin_bert_2019,
 abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
 author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
 keywords = {Computer Science - Computation and Language},
 language = {en},
 month = {May},
 note = {arXiv:1810.04805 [cs]},
 publisher = {arXiv},
 shorttitle = {{BERT}},
 title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
 url = {http://arxiv.org/abs/1810.04805},
 urldate = {2024-04-21},
 year = {2019}
}

@article{dwork_calibrating_2006,
 abstract = {We continue a line of research initiated in [10, 11] on privacypreserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.},
 author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
 doi = {https://doi.org/10.1007/11681878_14},
 journal = {Springer, Berlin, Heidelberg},
 keywords = {differential privacy},
 language = {en},
 series = {Theory of {Cryptography}},
 title = {Calibrating {Noise} to {Sensitivity} in {Private} {Data} {Analysis}},
 url = {https://doi.org/10.1007/11681878_14},
 volume = {3876},
 year = {2006}
}

@article{dwork_privacy-preserving_2018,
 abstract = {Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classiﬁcation and regression.},
 author = {Dwork, Cynthia and Feldman, Viltaly},
 journal = {31st Annual Conference on Learning Theory},
 language = {en},
 number = {Proceedings of Machine Learning Research},
 title = {Privacy-preserving {Prediction}},
 url = {http://proceedings.mlr.press/v75/dwork18a/dwork18a.pdf},
 volume = {75},
 year = {2018}
}

@article{figueira_survey_2022,
 abstract = {Synthetic data consists of artiﬁcially generated data. When data are scarce, or of poor quality, synthetic data can be used, for example, to improve the performance of machine learning models. Generative adversarial networks (GANs) are a state-of-the-art deep generative models that can generate novel synthetic samples that follow the underlying data distribution of the original dataset. Reviews on synthetic data generation and on GANs have already been written. However, none in the relevant literature, to the best of our knowledge, has explicitly combined these two topics. This survey aims to ﬁll this gap and provide useful material to new researchers in this ﬁeld. That is, we aim to provide a survey that combines synthetic data generation and GANs, and that can act as a good and strong starting point for new researchers in the ﬁeld, so that they have a general overview of the key contributions and useful references. We have conducted a review of the state-of-the-art by querying four major databases: Web of Sciences (WoS), Scopus, IEEE Xplore, and ACM Digital Library. This allowed us to gain insights into the most relevant authors, the most relevant scientiﬁc journals in the area, the most cited papers, the most signiﬁcant research areas, the most important institutions, and the most relevant GAN architectures. GANs were thoroughly reviewed, as well as their most common training problems, their most important breakthroughs, and a focus on GAN architectures for tabular data. Further, the main algorithms for generating synthetic data, their applications and our thoughts on these methods are also expressed. Finally, we reviewed the main techniques for evaluating the quality of synthetic data (especially tabular data) and provided a schematic overview of the information presented in this paper.},
 author = {Figueira, Alvaro and Vaz, Bruno},
 copyright = {https://creativecommons.org/licenses/by/4.0/},
 doi = {10.3390/math10152733},
 issn = {2227-7390},
 journal = {Mathematics},
 language = {en},
 month = {August},
 number = {15},
 pages = {2733},
 title = {Survey on {Synthetic} {Data} {Generation}, {Evaluation} {Methods} and {GANs}},
 url = {https://www.mdpi.com/2227-7390/10/15/2733},
 urldate = {2024-06-28},
 volume = {10},
 year = {2022}
}

@article{fraile_navarro_clinical_2023,
 abstract = {Background: Natural Language Processing (NLP) applications have developed over the past years in various fields including its application to clinical free text for named entity recognition and relation extraction. However, there has been rapid developments the last few years that there’s currently no overview of it. Moreover, it is unclear how these models and tools have been translated into clinical practice. We aim to synthesize and review these developments.},
 author = {Fraile Navarro, David and Ijaz, Kiran and Rezazadegan, Dana and Rahimi-Ardabili, Hania and Dras, Mark and Coiera, Enrico and Berkovsky, Shlomo},
 doi = {10.1016/j.ijmedinf.2023.105122},
 issn = {13865056},
 journal = {International Journal of Medical Informatics},
 language = {en},
 month = {September},
 pages = {105122},
 shorttitle = {Clinical named entity recognition and relation extraction using natural language processing of medical free text},
 title = {Clinical named entity recognition and relation extraction using natural language processing of medical free text: {A} systematic review},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S1386505623001405},
 urldate = {2024-05-27},
 volume = {177},
 year = {2023}
}

@inproceedings{guan_generation_2018,
 abstract = {Machine learning (ML) and Natural Language Processing (NLP) have achieved remarkable success in many ﬁelds and have brought new opportunities and high expectation in the analyses of medical data. The most common type of medical data is the massive free-text electronic medical records (EMR). It is widely regarded that mining such massive data can bring up important information for improving medical practices as well as for possible new discoveries on complex diseases. However, the free EMR texts are lacking consistent standards, rich of private information, and limited in availability. Also, as they are accumulated from everyday practices, it is often hard to have a balanced number of samples for the types of diseases under study. These problems hinder the development of ML and NLP methods for EMR data analysis. To tackle these problems, we developed a model to generate synthetic text of EMRs called Medical Text Generative Adversarial Network or mtGAN. It is based on the GAN framework and is trained by the REINFORCE algorithm. It takes disease features as inputs and generates synthetic texts as EMRs for the corresponding diseases. We evaluate the model from micro-level, macro-level and applicationlevel on a Chinese EMR text dataset. The results show that the method has a good capacity to ﬁt real data and can generate realistic and diverse EMR samples. This provides a novel way to avoid potential leakage of patient privacy while still supply sufﬁcient well-controlled cohort data for developing downstream ML and NLP methods. It can also be used as a data augmentation method to assist studies based on real EMR data.},
 address = {Madrid, Spain},
 author = {Guan, Jiaqi and Li, Runzhe and Yu, Sheng and Zhang, Xuegong},
 booktitle = {2018 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
 doi = {10.1109/BIBM.2018.8621223},
 isbn = {978-1-5386-5488-0},
 language = {en},
 month = {December},
 pages = {374--380},
 publisher = {IEEE},
 title = {Generation of {Synthetic} {Electronic} {Medical} {Record} {Text}},
 url = {https://ieeexplore.ieee.org/document/8621223/},
 urldate = {2024-06-05},
 year = {2018}
}

@inproceedings{guan_generation_2018-1,
 abstract = {Machine learning (ML) and Natural Language Processing (NLP) have achieved remarkable success in many ﬁelds and have brought new opportunities and high expectation in the analyses of medical data. The most common type of medical data is the massive free-text electronic medical records (EMR). It is widely regarded that mining such massive data can bring up important information for improving medical practices as well as for possible new discoveries on complex diseases. However, the free EMR texts are lacking consistent standards, rich of private information, and limited in availability. Also, as they are accumulated from everyday practices, it is often hard to have a balanced number of samples for the types of diseases under study. These problems hinder the development of ML and NLP methods for EMR data analysis. To tackle these problems, we developed a model to generate synthetic text of EMRs called Medical Text Generative Adversarial Network or mtGAN. It is based on the GAN framework and is trained by the REINFORCE algorithm. It takes disease features as inputs and generates synthetic texts as EMRs for the corresponding diseases. We evaluate the model from micro-level, macro-level and applicationlevel on a Chinese EMR text dataset. The results show that the method has a good capacity to ﬁt real data and can generate realistic and diverse EMR samples. This provides a novel way to avoid potential leakage of patient privacy while still supply sufﬁcient well-controlled cohort data for developing downstream ML and NLP methods. It can also be used as a data augmentation method to assist studies based on real EMR data.},
 address = {Madrid, Spain},
 author = {Guan, Jiaqi and Li, Runzhe and Yu, Sheng and Zhang, Xuegong},
 booktitle = {2018 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
 doi = {10.1109/BIBM.2018.8621223},
 isbn = {978-1-5386-5488-0},
 language = {en},
 month = {December},
 pages = {374--380},
 publisher = {IEEE},
 title = {Generation of {Synthetic} {Electronic} {Medical} {Record} {Text}},
 url = {https://ieeexplore.ieee.org/document/8621223/},
 urldate = {2024-06-04},
 year = {2018}
}

@misc{huang_clinicalbert_2020,
 abstract = {Clinical notes contain information about patients beyond structured data such as lab values or medications. However, clinical notes have been underused relative to structured data, because notes are highdimensional and sparse. We aim to develop and evaluate a continuous representation of clinical notes. Given this representation, our goal is to predict 30-day hospital readmission at various timepoints of admission, including early stages and at discharge. We apply bidirectional encoder representations from transformers (bert) to clinical text. Publicly-released bert parameters are trained on standard corpora such as Wikipedia and BookCorpus, which differ from clinical text. We therefore pre-train bert using clinical notes and finetune the network for the task of predicting hospital readmission. This defines ClinicalBERT. ClinicalBERT uncovers high-quality relationships between medical concepts, as judged by physicians. ClinicalBERT outperforms various baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit on various clinically-motivated metrics. The attention weights of ClinicalBERT can also be used to interpret predictions. To facilitate research, we open-source model parameters, and scripts for training and evaluation. ClinicalBERT is a flexible framework to represent clinical notes. It improves on previous clinical text processing methods and with little engineering can be adapted to other clinical predictive tasks.},
 author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:1904.05342 [cs]},
 publisher = {arXiv},
 shorttitle = {{ClinicalBERT}},
 title = {{ClinicalBERT}: {Modeling} {Clinical} {Notes} and {Predicting} {Hospital} {Readmission}},
 url = {http://arxiv.org/abs/1904.05342},
 urldate = {2024-01-02},
 year = {2020}
}

@article{idnay_systematic_2021,
 abstract = {Objective: We conducted a systematic review to assess the effect of natural language processing (NLP) systems in improving the accuracy and efﬁciency of eligibility prescreening during the clinical research recruitment process. Materials and Methods: Guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) standards of quality for reporting systematic reviews, a protocol for study eligibility was developed a priori and registered in the PROSPERO database. Using predetermined inclusion criteria, studies published from database inception through February 2021 were identiﬁed from 5 databases. The Joanna Briggs Institute Critical Appraisal Checklist for Quasi-experimental Studies was adapted to determine the study quality and the risk of bias of the included articles.
Results: Eleven studies representing 8 unique NLP systems met the inclusion criteria. These studies demonstrated moderate study quality and exhibited heterogeneity in the study design, setting, and intervention type. All 11 studies evaluated the NLP system’s performance for identifying eligible participants; 7 studies evaluated the system’s impact on time efﬁciency; 4 studies evaluated the system’s impact on workload; and 2 studies evaluated the system’s impact on recruitment. Discussion: NLP systems in clinical research eligibility prescreening are an understudied but promising ﬁeld that requires further research to assess its impact on real-world adoption. Future studies should be centered on continuing to develop and evaluate relevant NLP systems to improve enrollment into clinical studies.
Conclusion: Understanding the role of NLP systems in improving eligibility prescreening is critical to the advancement of clinical research recruitment.},
 author = {Idnay, Betina and Dreisbach, Caitlin and Weng, Chunhua and Schnall, Rebecca},
 copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
 doi = {10.1093/jamia/ocab228},
 issn = {1527-974X},
 journal = {Journal of the American Medical Informatics Association},
 language = {en},
 month = {December},
 number = {1},
 pages = {197--206},
 title = {A systematic review on natural language processing systems for eligibility prescreening in clinical research},
 url = {https://academic.oup.com/jamia/article/29/1/197/6415462},
 urldate = {2024-05-27},
 volume = {29},
 year = {2021}
}

@book{jarmul_practical_2023,
 author = {Jarmul, Katharine},
 edition = {1st},
 month = {June},
 publisher = {O'Reilly Media},
 title = {Practical {Data} {Privacy}: {Enhancing} {Privacy} and {Security} in {Data}},
 year = {2023}
}

@inproceedings{johnson_deidentification_2020,
 abstract = {The ability of caregivers and investigators to share patient data is fundamental to many areas of clinical practice and biomedical research. Prior to sharing, it is often necessary to remove identifiers such as names, contact details, and dates in order to protect patient privacy. Deidentification, the process of removing identifiers, is challenging, however. High-quality annotated data for developing models is scarce; many target identifiers are highly heterogenous (for example, there are uncountable variations of patient names); and in practice anything less than perfect sensitivity may be considered a failure. As a result, patient data is often withheld when sharing would be beneficial, and identifiable patient data is often divulged when a deidentified version would suffice.},
 address = {Toronto Ontario Canada},
 author = {Johnson, Alistair E. W. and Bulgarelli, Lucas and Pollard, Tom J.},
 booktitle = {Proceedings of the {ACM} {Conference} on {Health}, {Inference}, and {Learning}},
 doi = {10.1145/3368555.3384455},
 isbn = {978-1-4503-7046-2},
 language = {en},
 month = {April},
 pages = {214--221},
 publisher = {ACM},
 title = {Deidentification of free-text medical records using pre-trained bidirectional transformers},
 url = {https://dl.acm.org/doi/10.1145/3368555.3384455},
 urldate = {2024-06-03},
 year = {2020}
}

@article{jordan_selecting_2022,
 abstract = {Privacy protection for health data is more than simply stripping datasets of speciﬁc identiﬁers. Privacy protection increasingly means the application of privacy-enhancing technologies (PETs), also known as privacy engineering. Demands for the application of PETs are not yet met with ease of use or even understanding. This paper provides a scope of the current peer-reviewed evidence regarding the practical use or adoption of various PETs for managing health data privacy. We describe the state of knowledge of PETS for the use and exchange of health data speciﬁcally and build a practical perspective on the steps needed to improve the standardization of the application of PETs for diverse uses of health data.},
 author = {Jordan, Sara and Fontaine, Clara and Hendricks-Sturrup, Rachele},
 doi = {10.3389/fpubh.2022.814163},
 issn = {2296-2565},
 journal = {Frontiers in Public Health},
 language = {en},
 month = {March},
 pages = {814163},
 title = {Selecting {Privacy}-{Enhancing} {Technologies} for {Managing} {Health} {Data} {Use}},
 url = {https://www.frontiersin.org/articles/10.3389/fpubh.2022.814163/full},
 urldate = {2023-01-29},
 volume = {10},
 year = {2022}
}

@article{joshi_federated_2022,
 abstract = {Federated learning is the process of developing machine learning models over datasets distributed across data centers such as hospitals, clinical research labs, and mobile devices while preventing data leakage. This survey examines previous research and studies on federated learning in the healthcare sector across a range of use cases and applications. Our survey shows what challenges, methods, and applications a practitioner should be aware of in the topic of federated learning. This paper aims to lay out existing research and list the possibilities of federated learning for healthcare industries.},
 author = {Joshi, Madhura and Pal, Ankit and Sankarasubbu, Malaikannan},
 doi = {10.1145/3533708},
 issn = {2691-1957, 2637-8051},
 journal = {ACM Transactions on Computing for Healthcare},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2211.07893 [cs]},
 number = {4},
 pages = {1--36},
 title = {Federated {Learning} for {Healthcare} {Domain} - {Pipeline}, {Applications} and {Challenges}},
 url = {http://arxiv.org/abs/2211.07893},
 urldate = {2024-06-06},
 volume = {3},
 year = {2022}
}

@misc{kingma_adam_2017,
 abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
 author = {Kingma, Diederik P. and Ba, Jimmy},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {January},
 note = {arXiv:1412.6980 [cs]},
 publisher = {arXiv},
 shorttitle = {Adam},
 title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
 url = {http://arxiv.org/abs/1412.6980},
 urldate = {2024-04-21},
 year = {2017}
}

@misc{konecny_federated_2017,
 abstract = {Federated Learning is a machine learning setting where the goal is to train a highquality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efﬁciency is of the utmost importance.},
 author = {Konečný, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richtárik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
 keywords = {Computer Science - Machine Learning, federated learning, lr-supplemental},
 language = {en},
 month = {October},
 note = {arXiv:1610.05492 [cs]},
 publisher = {arXiv},
 shorttitle = {Federated {Learning}},
 title = {Federated {Learning}: {Strategies} for {Improving} {Communication} {Efficiency}},
 url = {http://arxiv.org/abs/1610.05492},
 urldate = {2024-04-16},
 year = {2017}
}

@misc{lange_closing_2020,
 abstract = {Exploiting natural language processing in the clinical domain requires de-identiﬁcation, i.e., anonymization of personal information in texts. However, current research considers de-identiﬁcation and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identiﬁcation on other tasks. In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identiﬁcation and concept extraction. In particular, we propose a stacked model with restricted access to privacy-sensitive information and a multitask model. We set the new state of the art on benchmark datasets in English (96.1\% F1 for de-identiﬁcation and 88.9\% F1 for concept extraction) and Spanish (91.4\% F1 for concept extraction).},
 author = {Lange, Lukas and Adel, Heike and Strötgen, Jannik},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2005.09397 [cs]},
 publisher = {arXiv},
 shorttitle = {Closing the {Gap}},
 title = {Closing the {Gap}: {Joint} {De}-{Identification} and {Concept} {Extraction} in the {Clinical} {Domain}},
 url = {http://arxiv.org/abs/2005.09397},
 urldate = {2023-12-26},
 year = {2020}
}

@article{lee_natural_2018,
 abstract = {A variety of methods existing for generating synthetic electronic health records (EHRs), but they are not capable of generating unstructured text, like emergency department (ED) chief complaints, history of present illness or progress notes. Here, we use the encoder-decoder model, a deep learning algorithm that features in many contemporary machine translation systems, to generate synthetic chief complaints from discrete variables in EHRs, like age group, gender, and discharge diagnosis. After being trained end-to-end on authentic records, the model can generate realistic chief complaint text that preserves much of the epidemiological information in the original data. As a side effect of the model’s optimization goal, these synthetic chief complaints are also free of relatively uncommon abbreviation and misspellings, and they include none of the personally-identifiable information (PII) that was in the training data, suggesting it may be used to support the de-identification of text in EHRs. When combined with algorithms like generative adversarial networks (GANs), our model could be used to generate fully-synthetic EHRs, facilitating data sharing between healthcare providers and researchers and improving our ability to develop machine learning methods tailored to the information in healthcare data.},
 author = {Lee, Scott H.},
 doi = {10.1038/s41746-018-0070-0},
 issn = {2398-6352},
 journal = {npj Digital Medicine},
 language = {en},
 month = {November},
 number = {1},
 pages = {63},
 title = {Natural language generation for electronic health records},
 url = {https://www.nature.com/articles/s41746-018-0070-0},
 urldate = {2024-06-05},
 volume = {1},
 year = {2018}
}

@article{lee_transfer_nodate,
 abstract = {Recent approaches based on artiﬁcial neural networks (ANNs) have shown promising results for named-entity recognition (NER). In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difﬁcult to obtain for the dataset on which the user wants to perform NER: label scarcity is particularly pronounced for patient note de-identiﬁcation, which is an instance of NER. In this work, we analyze to what extent transfer learning may address this issue. In particular, we demonstrate that transferring an ANN model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state-of-the-art results on two different datasets for patient note de-identiﬁcation.},
 author = {Lee, Ji Young and Dernoncourt, Franck and Szolovits, Peter},
 language = {en},
 title = {Transfer {Learning} for {Named}-{Entity} {Recognition} with {Neural} {Networks}}
}

@misc{li_split_2023,
 abstract = {Deep learning continues to rapidly evolve and is now demonstrating remarkable potential for numerous medical prediction tasks. However, realizing deep learning models that generalize across healthcare organizations is challenging. This is due, in part, to the inherent siloed nature of these organizations and patient privacy requirements. To address this problem, we illustrate how split learning can enable collaborative training of deep learning models across disparate and privately maintained health datasets, while keeping the original records and model parameters private. We introduce a new privacy-preserving distributed learning framework that offers a higher level of privacy compared to conventional federated learning. We use several biomedical imaging and electronic health record (EHR) datasets to show that deep learning models trained via split learning can achieve highly similar performance to their centralized and federated counterparts while greatly improving computational efficiency and reducing privacy risks.},
 author = {Li, Zhuohang and Yan, Chao and Zhang, Xinmeng and Gharibi, Gharib and Yin, Zhijun and Jiang, Xiaoqian and Malin, Bradley A.},
 keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
 language = {en},
 month = {August},
 note = {arXiv:2308.11027 [cs]},
 publisher = {arXiv},
 title = {Split {Learning} for {Distributed} {Collaborative} {Training} of {Deep} {Learning} {Models} in {Health} {Informatics}},
 url = {http://arxiv.org/abs/2308.11027},
 urldate = {2024-01-02},
 year = {2023}
}

@article{little_federated_2023,
 abstract = {Objectives The objective was to review current research and practices for using FL to generate synthetic data and determine the extent to which research has been undertaken, the methods and evaluation practices used, and any research gaps.
Methods A scoping review was conducted to systematically map and describe the published literature on the use of FL to generate synthetic data. Relevant studies were identified through online databases and the findings are described, grouped, and summarised. Information extracted included article characteristics, documenting the type of data that is synthesised, the model architecture and the methods (if any) used to evaluate utility and privacy risk.
Results A total of 69 articles were included in the scoping review; all were published between 2018 and 2023 with two thirds (46) in 2022. 30\% (21) were focussed on synthetic data generation as the main model output (with 6 of these generating tabular data), whereas 59\% (41) focussed on data augmentation. Of the 21 performing federated synthesis, all used deep learning methods (predominantly Generative Adversarial Networks) to generate the synthetic data.
Conclusions Federated synthesis is in its early days but shows promise as a method that can construct a global synthetic dataset without sharing any of the local client data. As a field in its infancy there are areas to explore in terms of the privacy risk associated with the various methods proposed, and more generally in how we measure those risks.},
 author = {Little, Claire and Elliot, Mark and Allmendinger, Richard},
 copyright = {http://creativecommons.org/licenses/by/4.0},
 doi = {10.23889/ijpds.v8i1.2158},
 issn = {2399-4908},
 journal = {International Journal of Population Data Science},
 language = {en},
 month = {October},
 number = {1},
 shorttitle = {Federated learning for generating synthetic data},
 title = {Federated learning for generating synthetic data: a scoping review},
 url = {https://ijpds.org/article/view/2158},
 urldate = {2024-06-06},
 volume = {8},
 year = {2023}
}

@book{ludwig_federated_2022,
 address = {Cham},
 doi = {10.1007/978-3-030-96896-0},
 editor = {Ludwig, Heiko and Baracaldo, Nathalie},
 isbn = {978-3-030-96895-3 978-3-030-96896-0},
 language = {en},
 publisher = {Springer International Publishing},
 shorttitle = {Federated {Learning}},
 title = {Federated {Learning}: {A} {Comprehensive} {Overview} of {Methods} and {Applications}},
 url = {https://link.springer.com/10.1007/978-3-030-96896-0},
 urldate = {2022-12-16},
 year = {2022}
}

@article{luo_biogpt_2022,
 abstract = {Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate f luent descriptions for biomedical terms.},
 author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
 copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
 doi = {10.1093/bib/bbac409},
 issn = {1467-5463, 1477-4054},
 journal = {Briefings in Bioinformatics},
 language = {en},
 month = {November},
 number = {6},
 pages = {bbac409},
 shorttitle = {{BioGPT}},
 title = {{BioGPT}: generative pre-trained transformer for biomedical text generation and mining},
 url = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbac409/6713511},
 urldate = {2024-05-27},
 volume = {23},
 year = {2022}
}

@misc{mcmahan_communication-efficient_2016,
 abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.},
 author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Agüera y},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:1602.05629 [cs]},
 publisher = {arXiv},
 title = {Communication-{Efficient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
 url = {http://arxiv.org/abs/1602.05629},
 urldate = {2024-06-05},
 year = {2016}
}

@misc{morris_text_2023,
 abstract = {How much private information do text embeddings reveal about the original text? We investigate the problem of embedding {\textbackslash}textit\{inversion\}, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a na{\textbackslash}"ive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover \$92{\textbackslash}\%\$ of \$32{\textbackslash}text\{-token\}\$ text inputs exactly. We train our model to decode text embeddings from two state-of-the-art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes. Our code is available on Github: {\textbackslash}href\{https://github.com/jxmorris12/vec2text\}\{github.com/jxmorris12/vec2text\}.},
 author = {Morris, John X. and Kuleshov, Volodymyr and Shmatikov, Vitaly and Rush, Alexander M.},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2310.06816 [cs]},
 publisher = {arXiv},
 title = {Text {Embeddings} {Reveal} ({Almost}) {As} {Much} {As} {Text}},
 url = {http://arxiv.org/abs/2310.06816},
 urldate = {2023-12-03},
 year = {2023}
}

@article{mujtaba_clinical_2019,
 author = {Mujtaba, Ghulam and Shuib, Liyana and Idris, Norisma and Hoo, Wai Lam and Raj, Ram Gopal and Khowaja, Kamran and Shaikh, Khairunisa and Nweke, Henry Friday},
 doi = {10.1016/j.eswa.2018.09.034},
 issn = {09574174},
 journal = {Expert Systems with Applications},
 language = {en},
 month = {February},
 pages = {494--520},
 shorttitle = {Clinical text classification research trends},
 title = {Clinical text classification research trends: {Systematic} literature review and open issues},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418306110},
 urldate = {2024-05-27},
 volume = {116},
 year = {2019}
}

@misc{office_of_civil_rights_summary_2022,
 author = {Office of Civil Rights},
 journal = {U.S Department of Health and Human Services},
 title = {Summary of the {HIPAA} {Privacy} {Rule}},
 url = {https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html},
 year = {2022}
}

@misc{office_of_civil_rights_summary_2022-1,
 author = {Office of Civil Rights},
 journal = {U.S Department of Health and Human Services},
 title = {Summary of the {HIPAA} {Security} {Rule}},
 url = {https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html},
 year = {2022}
}

@misc{papernot_semi-supervised_2017,
 abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.},
 author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
 keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
 language = {en},
 month = {March},
 note = {arXiv:1610.05755 [cs, stat]},
 publisher = {arXiv},
 title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
 url = {http://arxiv.org/abs/1610.05755},
 urldate = {2023-12-26},
 year = {2017}
}

@article{radford_improving_2018,
 abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
 author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
 language = {en},
 title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
 year = {2018}
}

@inproceedings{ren_generative_2022,
 abstract = {In recent years, automatic computational systems based on deep learning are widely used in medical fields, such as automatic diagnosing and disease prediction. Most of these systems are designed for data sufficient scenarios. However, due to the disease rarity or privacy, the medical data are always insufficient. When applying these data-hungry deep learning models with insufficient data, it is likely to lead to issues of over-fitting and cause serious performance problems. Many data augmentation methods have been proposed to solve the data insufficiency problem, such as using GAN (Generative Adversarial Networks) to generate training data. However, the augmented data usually contains lots of noise. Directly using them to train sensitive medical models is very difficult to achieve satisfactory results.},
 address = {Washington DC USA},
 author = {Ren, Houxing and Wang, Jingyuan and Zhao, Wayne Xin},
 booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
 doi = {10.1145/3534678.3539020},
 isbn = {978-1-4503-9385-0},
 language = {en},
 month = {August},
 pages = {3810--3818},
 publisher = {ACM},
 title = {Generative {Adversarial} {Networks} {Enhanced} {Pre}-training for {Insufficient} {Electronic} {Health} {Records} {Modeling}},
 url = {https://dl.acm.org/doi/10.1145/3534678.3539020},
 urldate = {2024-06-06},
 year = {2022}
}

@article{salvi_multi-modality_2024,
 abstract = {Healthcare traditionally relies on single-modality approaches, which limit the information available for medical decisions. However, advancements in technology and the availability of diverse data sources have made it feasible to integrate multiple modalities and gain a more comprehensive understanding of patients’ conditions. Multi-modality approaches involve fusing and analyzing various data types, including medical images, bio­ signals, clinical records, and other relevant sources.},
 author = {Salvi, Massimo and Loh, Hui Wen and Seoni, Silvia and Barua, Prabal Datta and García, Salvador and Molinari, Filippo and Acharya, U. Rajendra},
 doi = {10.1016/j.inffus.2023.102134},
 issn = {15662535},
 journal = {Information Fusion},
 language = {en},
 month = {March},
 pages = {102134},
 shorttitle = {Multi-modality approaches for medical support systems},
 title = {Multi-modality approaches for medical support systems: {A} systematic review of the last decade},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253523004505},
 urldate = {2024-05-27},
 volume = {103},
 year = {2024}
}

@article{samarati_protecting_1997,
 abstract = {Today's globally networked society places great demand on the dissemination and sharing of person-speci c data. Situations where aggregate statistical information was once the reporting norm now rely heavily on the transfer of microscopically detailed transaction and encounter information. This happens at a time when more and more historically public information is also electronically available. When these data are linked together, they provide an electronic shadow of a person or organization that is as identifying and personal as a ngerprint, even when the sources of the information contains no explicit identi ers, such as name and phone number. In order to protect the anonymity of individuals to whom released data refer, data holders often remove or encrypt explicit identi ers such as names, addresses and phone numbers. However, other distinctive data, which we term quasi-identi ers, often combine uniquely and can be linked to publicly available information to re-identify individuals.},
 author = {Samarati, Pierangela and Sweeney, Latanya},
 doi = {https://doi.org/10.1184/R1/6625469.v1},
 journal = {SRI International},
 language = {en},
 title = {Protecting {Privacy} when {Disclosing} {Information}: k-{Anonymity} and {Its} {Enforcement} through {Generalization} and {Suppression}},
 year = {1997}
}

@article{sufi_generative_2024,
 abstract = {GPT (Generative Pre-trained Transformer) represents advanced language models that have significantly reshaped the academic writing landscape. These sophisticated language models offer invaluable support throughout all phases of research work, facilitating idea generation, enhancing drafting processes, and overcoming challenges like writer’s block. Their capabilities extend beyond conventional applications, contributing to critical analysis, data augmentation, and research design, thereby elevating the efficiency and quality of scholarly endeavors. Strategically narrowing its focus, this review explores alternative dimensions of GPT and LLM applications, specifically data augmentation and the generation of synthetic data for research. Employing a meticulous examination of 412 scholarly works, it distills a selection of 77 contributions addressing three critical research questions: (1) GPT on Generating Research data, (2) GPT on Data Analysis, and (3) GPT on Research Design. The systematic literature review adeptly highlights the central focus on data augmentation, encapsulating 48 pertinent scholarly contributions, and extends to the proactive role of GPT in critical analysis of research data and shaping research design. Pioneering a comprehensive classification framework for “GPT’s use on Research Data”, the study classifies existing literature into six categories and 14 sub-categories, providing profound insights into the multifaceted applications of GPT in research data. This study meticulously compares 54 pieces of literature, evaluating research domains, methodologies, and advantages and disadvantages, providing scholars with profound insights crucial for the seamless integration of GPT across diverse phases of their scholarly pursuits.},
 author = {Sufi, Fahim},
 copyright = {https://creativecommons.org/licenses/by/4.0/},
 doi = {10.3390/info15020099},
 issn = {2078-2489},
 journal = {Information},
 language = {en},
 month = {February},
 number = {2},
 pages = {99},
 shorttitle = {Generative {Pre}-{Trained} {Transformer} ({GPT}) in {Research}},
 title = {Generative {Pre}-{Trained} {Transformer} ({GPT}) in {Research}: {A} {Systematic} {Review} on {Data} {Augmentation}},
 url = {https://www.mdpi.com/2078-2489/15/2/99},
 urldate = {2024-07-01},
 volume = {15},
 year = {2024}
}

@article{sweeney_only_2015,
 author = {Sweeney, Latanya},
 language = {en},
 title = {Only {You}, {Your} {Doctor}, and {Many} {Others} {May} {Know}},
 url = {https://techscience.org/a/2015092903/},
 year = {2015}
}

@article{sweeney_re-identification_2019,
 abstract = {Researchers are increasingly asked to share research data as part of publication and funding processes and to maximize the benefits of publicly funded research. The Safe Harbor provision of the U.S. Health Information Portability and Accountability Act (HIPAA) offers guidance to researchers by prescribing how to redact data for public sharing. For example, the provision requires removing explicit identifiers (such as name, address and other personally identifiable information), reporting dates in years, and reducing some or all digits of a postal (or ZIP) code. Is this sufficient? Can research participants still be re-identified in research data that adhere to the HIPAA Safe Harbor standard? In 2006, researchers collected air and dust samples and interviewed residents of 50 homes from Bolinas and Richmond (Atchison Village and Liberty Village), California, to analyze the residents’ exposure to pollutants. The study, known as the Northern California Household Exposure Study [1], led to publications that have been cited hundreds of times. We conducted experiments with separate “attacker” and “scorer” teams to see whether we could identify study participants from two versions of the data redacted beyond the HIPAA standard, one in which all dates were reported in ranges of 10 or 20 years and another in which a study participant’s birth year was reported exactly. The attackers were blinded to the names and addresses of the participants, and the scorers were blinded to the strategy.},
 author = {Sweeney, Latanya and Yoo, Ji Su and Perovich, Laura and Boronow, Katherine E and Brown, Phil and Brody, Julia Green},
 language = {en},
 title = {Re-identification {Risks} in {HIPAA} {Safe} {Harbor} {Data}: {A} study of data from one environmental health study},
 year = {2019}
}

@misc{thakkar_understanding_2020,
 abstract = {Recent works have shown that generative sequence models (e.g., language models) have a tendency to memorize rare or unique sequences in the training data. Since useful models are often trained on sensitive data, to ensure the privacy of the training data it is critical to identify and mitigate such unintended memorization. Federated Learning (FL) has emerged as a novel framework for large-scale distributed learning tasks. However, it differs in many aspects from the well-studied central learning setting where all the data is stored at the central server. In this paper, we initiate a formal study to understand the effect of different components of canonical FL on unintended memorization in trained models, comparing with the central learning setting. Our results show that several differing components of FL play an important role in reducing unintended memorization. Speciﬁcally, we observe that the clustering of data according to users—which happens by design in FL—has a signiﬁcant effect in reducing such memorization, and using the method of Federated Averaging for training causes a further reduction. We also show that training with a strong user-level differential privacy guarantee results in models that exhibit the least amount of unintended memorization.},
 author = {Thakkar, Om and Ramaswamy, Swaroop and Mathews, Rajiv and Beaufays, Françoise},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2006.07490 [cs, stat]},
 publisher = {arXiv},
 title = {Understanding {Unintended} {Memorization} in {Federated} {Learning}},
 url = {http://arxiv.org/abs/2006.07490},
 urldate = {2024-01-03},
 year = {2020}
}

@article{timmermans_world_2010,
 abstract = {Standards and standardization aim to render the world equivalent across cultures, time, and geography. Standards are ubiquitous but underappreciated tools for regulating and organizing social life in modernity, and they lurk in the background of many sociological works. Reviewing the relevance of standards and standardization in diverse theoretical traditions and sociological subﬁelds, we point to the emergence and institutionalization of standards, the difﬁculties of making standards work, resistance to standardization, and the multiple outcomes of standards. Rather than associating standardization with totalizing narratives of globalization or dehumanization, we call for careful empirical analysis of the speciﬁc and unintended consequences of different sorts of standards operating in distinct social domains.},
 author = {Timmermans, Stefan and Epstein, Steven},
 doi = {10.1146/annurev.soc.012809.102629},
 issn = {0360-0572, 1545-2115},
 journal = {Annual Review of Sociology},
 keywords = {standards},
 language = {en},
 month = {June},
 number = {1},
 pages = {69--89},
 shorttitle = {A {World} of {Standards} but not a {Standard} {World}},
 title = {A {World} of {Standards} but not a {Standard} {World}: {Toward} a {Sociology} of {Standards} and {Standardization}},
 url = {https://www.annualreviews.org/doi/10.1146/annurev.soc.012809.102629},
 urldate = {2024-02-21},
 volume = {36},
 year = {2010}
}

@misc{touvron_llama_2023,
 abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
 author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
 language = {en},
 month = {July},
 note = {arXiv:2307.09288 [cs]},
 publisher = {arXiv},
 shorttitle = {Llama 2},
 title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
 url = {http://arxiv.org/abs/2307.09288},
 urldate = {2024-04-21},
 year = {2023}
}

@article{tovino_not_2022,
 author = {Tovino, Stacey A.},
 journal = {Duke Law Journal},
 month = {February},
 number = {5},
 pages = {985--1046},
 title = {Not {So} {Private}},
 volume = {71},
 year = {2022}
}

@article{vakili_downstream_2022,
 abstract = {Automatic de-identification is a cost-effective and straightforward way of removing large amounts of personally identifiable information from large and sensitive corpora. However, these systems also introduce errors into datasets due to their imperfect precision. These corruptions of the data may negatively impact the utility of the de-identified dataset. This paper de-identifies a very large clinical corpus in Swedish either by removing entire sentences containing sensitive data or by replacing sensitive words with realistic surrogates. These two datasets are used to perform domain adaptation of a general Swedish BERT model. The impact of the de-identification techniques is assessed by training and evaluating the models using six clinical downstream tasks. The results are then compared to a similar BERT model domain-adapted using an unaltered version of the clinical corpus. The results show that using an automatically de-identified corpus for domain adaptation does not negatively impact downstream performance. We argue that automatic de-identification is an efficient way of reducing the privacy risks of domain-adapted models and that the models created in this paper should be safe to distribute to other academic researchers.},
 author = {Vakili, Thomas and Lamproudis, Anastasios and Henriksson, Aron and Dalianis, Hercules},
 language = {en},
 title = {Downstream {Task} {Performance} of {BERT} {Models} {Pre}-{Trained} {Using} {Automatically} {De}-{Identified} {Clinical} {Data}},
 url = {https://aclanthology.org/2022.lrec-1.451},
 year = {2022}
}

@misc{van_der_maaten_trade-offs_2020,
 abstract = {Machine learning models leak information about their training data every time they reveal a prediction. This is problematic when the training data needs to remain private. Private prediction methods limit how much information about the training data is leaked by each prediction. Private prediction can also be achieved using models that are trained by private training methods. In private prediction, both private training and private prediction methods exhibit trade-offs between privacy, privacy failure probability, amount of training data, and inference budget. Although these trade-offs are theoretically well-understood, they have hardly been studied empirically. This paper presents the ﬁrst empirical study into the trade-offs of private prediction. Our study sheds light on which methods are best suited for which learning setting. Perhaps surprisingly, we ﬁnd private training methods outperform private prediction methods in a wide range of private prediction settings.},
 author = {van der Maaten, Laurens and Hannun, Awni},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
 language = {en},
 month = {July},
 note = {arXiv:2007.05089 [cs, stat]},
 publisher = {arXiv},
 title = {The {Trade}-{Offs} of {Private} {Prediction}},
 url = {http://arxiv.org/abs/2007.05089},
 urldate = {2024-01-23},
 year = {2020}
}

@misc{vaswani_attention_2023,
 abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {August},
 note = {arXiv:1706.03762 [cs]},
 publisher = {arXiv},
 title = {Attention {Is} {All} {You} {Need}},
 url = {http://arxiv.org/abs/1706.03762},
 urldate = {2024-04-21},
 year = {2023}
}

@misc{vepakomma_split_2018,
 abstract = {Can health entities collaboratively train deep learning models without sharing sensitive raw data? This paper proposes several conﬁgurations of a distributed deep learning method called SplitNN to facilitate such collaborations. SplitNN does not share raw data or model details with collaborating institutions. The proposed conﬁgurations of splitNN cater to practical settings of i) entities holding different modalities of patient data, ii) centralized and local health entities collaborating on multiple tasks and iii) learning without sharing labels. We compare performance and resource efﬁciency trade-offs of splitNN and other distributed deep learning methods like federated learning, large batch synchronous stochastic gradient descent and show highly encouraging results for splitNN.},
 author = {Vepakomma, Praneeth and Gupta, Otkrist and Swedish, Tristan and Raskar, Ramesh},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, lr-supplemental},
 language = {en},
 month = {December},
 note = {arXiv:1812.00564 [cs, stat]},
 publisher = {arXiv},
 shorttitle = {Split learning for health},
 title = {Split learning for health: {Distributed} deep learning without sharing raw patient data},
 url = {http://arxiv.org/abs/1812.00564},
 urldate = {2024-04-16},
 year = {2018}
}

@article{yang_large_2022,
 abstract = {Abstract

There is an increasing interest in developing artificial intelligence (AI) systems to process and interpret electronic health records (EHRs). Natural language processing (NLP) powered by pretrained language models is the key technology for medical AI systems utilizing clinical narratives. However, there are few clinical language models, the largest of which trained in the clinical domain is comparatively small at 110 million parameters (compared with billions of parameters in the general domain). It is not clear how large clinical language models with billions of parameters can help medical AI systems utilize unstructured EHRs. In this study, we develop from scratch a large clinical language model—GatorTron—using {\textgreater}90 billion words of text (including {\textgreater}82 billion words of de-identified clinical text) and systematically evaluate it on five clinical NLP tasks including clinical concept extraction, medical relation extraction, semantic textual similarity, natural language inference (NLI), and medical question answering (MQA). We examine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could benefit these NLP tasks. GatorTron models scale up the clinical language model from 110 million to 8.9 billion parameters and improve five clinical NLP tasks (e.g., 9.6\% and 9.5\% improvement in accuracy for NLI and MQA), which can be applied to medical AI systems to improve healthcare delivery. The GatorTron models are publicly available at:
https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron\_og
.},
 author = {Yang, Xi and Chen, Aokun and PourNejatian, Nima and Shin, Hoo Chang and Smith, Kaleb E. and Parisien, Christopher and Compas, Colin and Martin, Cheryl and Costa, Anthony B. and Flores, Mona G. and Zhang, Ying and Magoc, Tanja and Harle, Christopher A. and Lipori, Gloria and Mitchell, Duane A. and Hogan, William R. and Shenkman, Elizabeth A. and Bian, Jiang and Wu, Yonghui},
 doi = {10.1038/s41746-022-00742-2},
 issn = {2398-6352},
 journal = {npj Digital Medicine},
 language = {en},
 month = {December},
 number = {1},
 pages = {194},
 title = {A large language model for electronic health records},
 url = {https://www.nature.com/articles/s41746-022-00742-2},
 urldate = {2024-05-27},
 volume = {5},
 year = {2022}
}

@misc{yeom_privacy_2018,
 abstract = {Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak speciﬁc private information in the training data to an attacker, either through the models’ structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overﬁtting and inﬂuence might play a role.},
 author = {Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
 keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, MIA, Statistics - Machine Learning, differential privacy, influence, membership advantage, model attacks, overfitting, stability},
 language = {en},
 month = {May},
 note = {arXiv:1709.01604 [cs, stat]},
 publisher = {arXiv},
 shorttitle = {Privacy {Risk} in {Machine} {Learning}},
 title = {Privacy {Risk} in {Machine} {Learning}: {Analyzing} the {Connection} to {Overfitting}},
 url = {http://arxiv.org/abs/1709.01604},
 urldate = {2024-02-28},
 year = {2018}
}

@article{yoo_risks_2018,
 abstract = {Forty-eight states in the United States collect statewide inpatient discharge data that include personal health information of each patient’s hospital visit [1]. A 2013 survey found that 33 of those states subsequently sold or otherwise disclosed copies of the data, but only three states de-identified data consistent with the standards established under the Health Information Portability and Accountability Act (HIPAA), the U.S. federal regulation that dictates the rules by which personal health information is shared [2]. While states are not mandated to follow HIPAA when de-identifying their data, many states still use a version of HIPAA to de-identify discharge data. Did the other 30 states put the privacy of personal health data at risk? To answer this question, Latanya Sweeney tested whether Washington State’s hospital data was vulnerable to re-identification. The study showed that Washington State’s inpatient data allowed for the correct matching of 35 of 81 (or 43 percent) individuals identified in the news stories to the anonymized discharge data released by the states local newspaper stories to anonymized hospital visits [3]. After the study, Washington State improved its anonymization standard for publicly available data and added an application process for others to receive more detailed nonpublic discharge data. Despite this successful outcome, many states were not convinced that the same re-identification strategy would be successful on their datasets. One reason was a belief that Washington State was more vulnerable because it shared patient age in months, a practice not followed by many other states. Is this correct? Are other states exempt from this re-identification strategy? To find out, we repeated the approach on statewide health data from 2010 in Maine and 2011 in Vermont using a total of 291 local news stories.},
 author = {Yoo, Ji Su and Thaler, Alexandra and Sweeney, Latanya and Zang, Jinyan},
 journal = {. October},
 language = {en},
 title = {Risks to {Patient} {Privacy}: {A} {Re}- identification of {Patients} in {Maine} and {Vermont} {Statewide} {Hospital} {Data}},
 url = {https://techscience.org/a/2018100901/},
 year = {2018}
}

@article{yu_seqgan_2017,
 abstract = {As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difﬁcult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate signiﬁcant improvements over strong baselines.},
 author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
 doi = {10.1609/aaai.v31i1.10804},
 issn = {2374-3468, 2159-5399},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 keywords = {lr-supplemental},
 language = {en},
 month = {February},
 number = {1},
 shorttitle = {{SeqGAN}},
 title = {{SeqGAN}: {Sequence} {Generative} {Adversarial} {Nets} with {Policy} {Gradient}},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/10804},
 urldate = {2024-04-16},
 volume = {31},
 year = {2017}
}

@article{yuan_large_nodate,
 abstract = {The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32\% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12\%. Additionally, we present case studies to further illustrate the effectiveness of our approach and provide a deeper understanding of its underlying principles.},
 author = {Yuan, Jiayi and Tang, Ruixiang and Jiang, Xiaoqian and Hu, Xia},
 language = {en},
 title = {Large {Language} {Models} for {Healthcare} {Data} {Augmentation}: {An} {Example} on {Patient}-{Trial} {Matching}}
}
